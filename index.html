<!doctype html>
<html>
<head>
<meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">

<link type="text/css" rel="stylesheet" href="css/github2.css" >

<title>Probabilistic Graphical Models</title>
<script type="text/x-mathjax-config">MathJax.Hub.Config({tex2jax:{inlineMath:[['$$$','$$$']]}});</script><script src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>
</head>
<body>
<h1>Probabilistic Graphical Models</h1>

<p><em>Probabilistic Graphical Model</em> is a <em>probabilistic model</em> for which a graph denotes the conditional dependence structure between random variables. They are commonly used in probability theory, statics--particularly Bayesian statics--and machine learning.</p>

<h2>Preliminaries</h2>

<h3>Factors</h3>

<p><em>Factor</em> is a fundamental building block for defining distributions in high-dimensional spaces. Factor product defined as below
$$ \phi(a_1, b_1) \phi(b_1, c_1) = \phi(a_1, b_1, c_1) $$</p>

<h3>Reasoning Patterns</h3>

<ul>
<li>Causal Reasoning</li>
<li>Evidential Reasoning</li>
<li>Intercausal Reasoning</li>
</ul>


<h3>Independence</h3>

<p>For random variables $$$ X $$$, $$$ Y $$$, $$$ P \models X \perp Y $$$ if:</p>

<ul>
<li>$$$ P(X,Y) = P(X)P(Y) $$$</li>
<li>$$$ P(X \mid Y) = P(X) $$$</li>
<li>$$$ P(Y \mid X) = P(Y) $$$</li>
</ul>


<p>For random variables $$$ X $$$, $$$ Y $$$, $$$ Z $$$, $$$ P \models (X \perp Y \mid Z)$$$ if:</p>

<ul>
<li>$$$ P(X, Y \mid Z) = P(X \mid Z)P(Y \mid Z)$$$</li>
<li>$$$ P(X \mid Y, Z) = P(X \mid Z) $$$</li>
<li>$$$ P(Y \mid X, Z) = P(Y \mid Z) $$$</li>
<li>$$$ P(X, Y, Z) \propto \phi(X, Z) \phi(Y, Z)$$$</li>
</ul>


<h2>Bayesian Network</h2>

<p><em>Bayesian Network</em> is a directed acyclic graph(DAG), whose nodes represent the random variables $$$X_1$$$, $$$X_2$$$,…,$$$X_n$$$, each node $$$X_i$$$ represents a CPD $$$P(X_i \mid Par_G(X_i))$$$, the joint distribution represented by this graph is
$$ P(X_1, X_2, …, X_n) = \prod_i^n P(X_i \mid Par_G(X_i)) $$</p>

<p><em>Naive Bayes</em> is a bayesian network with very strong independence assumptions that every pair of features $$$X_i$$$ and $$$X_j$$$ are conditionally independent given class. that is
$$ P(X_i \perp X_j \mid C) $$</p>

<p><em>Naive Bayes</em> can be classified into <em>Bernoulli Naive Bayes</em> and <em>Multinomial Naive Bayes</em> according to the distribution over features.</p>

<p><em>Dynamic Bayesian Networks</em> are a compact representation for encoding structured distributions over arbitrarily long temporal trajectories, they make assumptions:</p>

<ul>
<li>Markov assumption</li>
<li>Time invariance</li>
</ul>


<p>Two equivalent views of <em>Bayesian Network</em> structure:</p>

<ul>
<li>Factorization: G allows P to be represented</li>
<li>I-map: Independencies encoded by G hold in P</li>
</ul>


<p>If P factorizes over a graph G, we can read from the graph independences that must hold in P (an independency map)</p>

<h2>Markov Network</h2>

<p><em>Pairwise Markov Network</em> is an undirected graph whose nodes represent the random variables $$$X_1$$$, $$$X_2$$$, …, $$$X_n$$$ and each edge $$$X_i - X_j$$$ is associated with a factor(potential) $$$ \phi_{ij}(X_i - X_j) $$$.</p>

<p>Two equivalent(for positive distributions) views of graph structure:</p>

<ul>
<li>Factorization: H allows P to be represented</li>
<li>I-map: Independencies encoded by H hold in P</li>
</ul>


<p>If P factorizes over a graph H, we can read from the graph independencies that must hold in P(an independency map)</p>
</body>
</html>
